import os
import torch
import torch.nn.functional as F
from transformers import BertTokenizer, BertForSequenceClassification


def load_kcbert_model(path="C:/Users/rhkda/PycharmProjects/PythonProject/src/topic/saved_kcbert_model"):
    if not os.path.exists(path):
        raise ValueError(f"ê²½ë¡œ '{path}'ì— ëª¨ë¸ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    model = BertForSequenceClassification.from_pretrained(path)
    tokenizer = BertTokenizer.from_pretrained(path)
    print(f"âœ… KcBERT ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ê°€ {path}ì—ì„œ ë¶ˆëŸ¬ì™€ì¡ŒìŠµë‹ˆë‹¤.")
    return model, tokenizer


def analyze_sentiment_kcbert(reviews):
    model, tokenizer = load_kcbert_model()
    results = []

    print(f"ğŸ“ ë¶„ì„í•  ë¦¬ë·° ê°œìˆ˜: {len(reviews)}")

    for review in reviews:
        # âœ… ë°ì´í„° ìˆœì„œì— ë§ì¶° ì²˜ë¦¬
        inputs = tokenizer(review, return_tensors="pt", truncation=True, padding=True)

        # ëª¨ë¸ì„ ì´ìš©í•œ ì˜ˆì¸¡
        with torch.no_grad():
            outputs = model(**inputs)
            probs = F.softmax(outputs.logits, dim=1)  # í™•ë¥ ê°’ ê³„ì‚°
            label = torch.argmax(probs).item()  # ë¼ë²¨ ì¶”ì¶œ (ê¸ì •/ë¶€ì •)
            confidence = probs[0][label].item()  # ìš°í˜¸ë„ (í™•ë¥ )

            # ê²°ê³¼ ì €ì¥
            results.append({
                "label": label,  # 0: ë¶€ì •, 1: ê¸ì •, -1: ë¶ˆí™•ì‹¤
                "confidence": confidence  # ì˜ˆì¸¡ í™•ì‹ ë„
            })


    print(f"âœ… ì´ {len(results)}ê°œì˜ ë¦¬ë·° ë¶„ì„ ì™„ë£Œ")
    return results
