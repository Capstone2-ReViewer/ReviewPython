import torch
import pandas as pd
from torch.utils.data import DataLoader, Dataset
from transformers import BertForSequenceClassification, BertTokenizer
import torch.optim as optim

# 1. ë°ì´í„° ë¡œë“œ
def load_data(file_path):
    df = pd.read_csv(file_path)
    df = df[df['label'] != -1]  # â— -1 ì œê±°
    return df['review'].tolist(), df['label'].tolist()

# 2. Dataset í´ë˜ìŠ¤ ì •ì˜
class ReviewDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, item):
        text = self.texts[item]
        label = self.labels[item]
        encoding = self.tokenizer.encode_plus(
            text,
            max_length=self.max_len,
            truncation=True,
            padding='max_length',
            add_special_tokens=True,
            return_tensors='pt'
        )
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# 3. ëª¨ë¸ ì €ì¥ í•¨ìˆ˜
def save_kcbert_model(model, tokenizer, path="./saved_kcbert_model"):
    model.save_pretrained(path)
    tokenizer.save_pretrained(path)
    print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {path}")

# 4. í•™ìŠµ í•¨ìˆ˜
def train_kcbert_model():
    file_path = './review_labeled.csv'
    texts, labels = load_data(file_path)

    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    dataset = ReviewDataset(texts, labels, tokenizer, max_len=128)
    dataloader = DataLoader(dataset, batch_size=4, shuffle=True)

    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
    optimizer = optim.Adam(model.parameters(), lr=1e-5)

    model.train()
    for epoch in range(4):
        total_loss = 0
        for i, batch in enumerate(dataloader):
            input_ids = batch['input_ids']
            attention_mask = batch['attention_mask']
            labels = batch['labels']

            optimizer.zero_grad()
            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        print(f"ğŸ“˜ Epoch {epoch + 1} ì™„ë£Œ - í‰ê·  Loss: {total_loss / len(dataloader):.4f}")

    save_kcbert_model(model, tokenizer)

# 5. ì‹¤í–‰
if __name__ == "__main__":
    train_kcbert_model()
